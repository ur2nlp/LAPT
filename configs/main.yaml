defaults:
    - _self_
    - training: l40_basic

# Reproducibility
seed: 1

# Language and model configuration
language_code: hy
hf_model: facebook/xglm-564M

# Dataset configuration
dataset:
  type: oscar
  language: ${language_code}
  cache_dir: "data/${language_code}_oscar"

max_length: 256
dev_size: 0.0025

# Model output configuration
output_dir: "models"  # Base directory for all model outputs (can override for cluster scratch space, etc.)
model_name: null  # Optional codename override (e.g., "yerevan" â†’ models/yerevan)

# FOCUS configuration for vocabulary specialization
focus:
  enabled: false
  tokenizer_path: null  # if provided, load existing tokenizer; if null, train new one
  vocab_size: 50000
  num_samples: 100000  # number of samples for tokenizer training & FOCUS
  dataset: null  # Optional: separate dataset config for FOCUS; if null, uses main training dataset