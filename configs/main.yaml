defaults:
    - _self_
    - training: l40_basic

# Reproducibility
seed: 1

# Language and model configuration
hf_model: facebook/xglm-564M

# Dataset configuration
dataset:
  type: oscar
  language: ???  # Required: specify language code (e.g., dataset.language=hy)
  cache_dir: "data/${dataset.language}_oscar"

# Model output configuration
output_dir: "models"  # Base directory for all model outputs (can override for cluster scratch space, etc.)
model_name: null  # Optional codename override (e.g., "yerevan" â†’ models/yerevan)
resume_from_checkpoint: null  # Optional path to checkpoint directory to resume training from

# FOCUS configuration for vocabulary specialization
focus:
  enabled: true
  tokenizer_path: null  # if provided, load existing tokenizer; if null, train new one
  vocab_size: 16384
  num_samples: 25000  # number of samples for tokenizer training & FOCUS
  dataset: null  # Optional: separate dataset config for FOCUS; if null, uses main training dataset
  inherit_additional_special_tokens: true  # Whether to inherit additional tokens like <madeupword0-6> from base model
  fasttext_model_min_count: 1  # Minimum count for FastText token embeddings (default: 4)
  character_coverage: 1.0  # SentencePiece character coverage (0-1): covers characters making up this fraction of corpus. Use 1.0 for small character sets, 0.9995 for rich sets (CJK)
