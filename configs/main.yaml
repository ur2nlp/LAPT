defaults:
    - _self_
    - training: l40_basic

# Reproducibility
seed: 1

# Language and model configuration
hf_model: facebook/xglm-564M

# Dataset configuration
dataset:
  type: oscar
  language: hy
  cache_dir: "data/${dataset.language}_oscar"

# Model output configuration
output_dir: "models"  # Base directory for all model outputs (can override for cluster scratch space, etc.)
model_name: null  # Optional codename override (e.g., "yerevan" â†’ models/yerevan)

# FOCUS configuration for vocabulary specialization
focus:
  enabled: true
  tokenizer_path: null  # if provided, load existing tokenizer; if null, train new one
  vocab_size: 32768
  num_samples: 1000000  # number of samples for tokenizer training & FOCUS
  dataset: null  # Optional: separate dataset config for FOCUS; if null, uses main training dataset
  inherit_additional_special_tokens: true  # Whether to inherit additional tokens like <madeupword0-6> from base model