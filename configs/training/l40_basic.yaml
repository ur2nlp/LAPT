name: l40_basic

max_length: 512

num_train_epochs: 1.0
max_steps: 100000
train_batch_size: 50
gradient_accumulation_steps: 1
learning_rate: 5e-5
lr_scheduler_type: linear
warmup_ratio: 0.0
warmup_steps: ${divide:${training.max_steps},200}
max_grad_norm: 1.0

logging_steps: ${divide:${training.max_steps},400}
eval_strategy: steps
metric_for_best_model: loss
eval_steps: ${divide:${training.max_steps},100}
eval_batch_size: 40
dev_size: 100000
save_steps: ${divide:${training.max_steps},100}
save_total_limit: 2
early_stopping_patience: 4

# Freezing parameters (optional)
freeze_main_model: true  # Set to true to freeze transformer layers initially
model_freeze_prefix: "model.layers"  # Prefix for parameters to freeze (XGLM transformer layers)
unfreeze_step_ratio: 0.1  # Ratio of max_steps at which to unfreeze (e.g., 0.1 = unfreeze after 10%)
