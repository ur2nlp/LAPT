name: l40_basic

max_length: 256

num_train_epochs: 1.0
max_steps: 100000
train_batch_size: 50
gradient_accumulation_steps: 1
learning_rate: 1e-5
lr_scheduler_type: linear
warmup_ratio: 0.005
max_grad_norm: 1.0

# Dropout for regularization
dropout: 0.15
attention_dropout: 0.15
activation_dropout: 0.0

logging_steps: ${divide:${training.max_steps},400}
eval_strategy: steps
metric_for_best_model: loss
eval_steps: ${divide:${training.max_steps},100}
eval_batch_size: 40
dev_size: 100000
save_steps: ${divide:${training.max_steps},100}
save_total_limit: 2
early_stopping_patience: null

# Freezing parameters (optional)
freeze_main_model: false  # Set to true to freeze transformer layers initially
model_freeze_prefix: "model.layers"  # Prefix for parameters to freeze (XGLM transformer layers)
unfreeze_step_ratio: null  # Ratio of max_steps at which to unfreeze (e.g., 0.1 = unfreeze after 10%)
